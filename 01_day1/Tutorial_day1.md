# Day 1. Handling NGS data: From raw reads to SNPs matrix <!-- omit from toc -->

## Table of contents <!-- omit from toc -->

- [1-1. Getting familiar with the Unix environment](#1-1-getting-familiar-with-the-unix-environment)
- [1-2. From raw sequences to mapped data](#1-2-from-raw-sequences-to-mapped-data)
  - [Pre-processing of raw data](#pre-processing-of-raw-data)
    - [FASTQ](#fastq)
  - [Conda environment](#conda-environment)
  - [Mapping sequence reads to a reference genome](#mapping-sequence-reads-to-a-reference-genome)
    - [SAM/BAM](#sambam)
- [1-3. Call variants with Stacks](#1-3-call-variants-with-stacks)
  - [Variant calling from whole genome re-sequencing data](#variant-calling-from-whole-genome-re-sequencing-data)
  - [SNPs calling](#snps-calling)


## 1-1. Getting familiar with the Unix environment
While we assume that you are comfortable working with the command line and under the Unix environment, we will refresh some of the most common commands that we will use during the tutorials. 

When you log in to the AWS server, you are positioned in the home directory. You can easily return to the home directory whenever you want by typing `cd` and pressing the `Enter/Return` key.

Let's start by creating a new directory called `scripts` within the home directory, and copy there using `cp` some files we will need later:
```bash
# make sure you are in the home directory
cd
# create a new directory
mkdir scripts
# go to the new directory
cd scripts
# copy all the files (*) within the 00_documents/ directory (from the Share directory) into the directory you are currently in
cp -r ~/Share/physalia_adaptation_course/00_documents/* .
# copy the 01_day1/01_scripts/ directory and its content (-r to refer to directory) into the directory you are currently in
cp -r ~/Share/physalia_adaptation_course/01_day1/01_scripts .

```
>Note that in this code block some lines start with a hash (#) symbol. The hash causes these lines to be ignored (not executed) and allows us to add comments to our code. Commenting your code is a very good practice that helps everyone, not just you, understand what it does, making easier to reuse the code in the future! 

You can use `ls` to list (explore) the files available in the current directory. 


Next, explore some files using the `less` command. Note that when using `less`, you can scroll-down by pressing the Space bar on your keyboard, and exit `less` with `q`:
```bash
# explore the content of the current directory
ls
# explore a file in the current directory
less popmap_2lin_day1.txt
# explore a file using a relative path
less 01_scripts/stacks_gstacks_2lin.sh
```
Another option is to go to the target directory using `cd targetFolder`, and then use `less targetFile` to display the file content. But, as you may have noticed, relative paths can save us some time!

If you want to print a few lines of a file to the screen (which won't disappear after exploring the file) use `head fileName`:
```bash
head popmap_2lin_day1.txt
```
The `head` command prints the first 10 lines by default. You can specify the number of lines you want with the flag `-n` followed by the number of lines, e.g. `-n 25`. Same for `tail`, which shows the last 10 lines of a file.

If you want to print an entire file on the screen (the standard output in this case) then use `cat`, and the equivalent command for compressed files is `zcat`, e.g. `zcat -n 1 ~/Share/physalia_adaptation_course/01_day1/02_genome/genome_mallotus_dummy.fasta.gz`. 

Make a copy of a file and assign a different name:
```bash
cp popmap_2lin_day1.txt test_popmap.txt
```
To edit a file, we will use the text editor called `nano`:
```bash
nano test_popmap.txt
``` 
To navigate through the lines of the file use the arrows on your keyboard. At the bottom of the screen there are shortcuts for different actions. 

For example, to modify the file and save it with a different name, write something in the first line, press the `Ctlr` and `o` keys (equivalent to `Ctlr + o`), type the new file name (`test_nano.txt`), press `Enter` and `y` to save the file under a different name. To exit nano, press `Ctlr + x`. 

If you want to make changes to a file and overwrite it, then press `Ctlr + x` and `y` (or `n` if not).

Now, let's delete/remove these files using `rm` and the wildcard symbol `*` which match against zero or more characters in a file (or directory) name:
```bash
rm test_*.txt
```
**WARNING! The command `rm` removes files and folders for good, and this cannot be undone. So please keep that in mind!**


## 1-2. From raw sequences to mapped data

### Pre-processing of raw data
The data we will use in the tutorials are derived from a population genomics study of capelin (*Mallotus villosus*), a small marine fish distributed across the North Atlantic, North Pacific and Arctic oceans. 

This dataset was generated by Cayuela et al. 2020: 
>Cayuela H, Rougemont Q, Laporte M, et al. Shared ancestral polymorphisms and chromosomal rearrangements as potential drivers of local adaptation in a marine fish. Mol Ecol. 2020; 29: 2379–2398. https://doi.org/10.1111/mec.15499

They generated DNA sequences using the IonTorrent platform. Raw data preprocessing (i.e., demultiplexing, adapter removal, read trimming, quality filtering) was already done for you. If interested, you can find example scripts for these preliminary steps in `01_day1/01_scripts`.

The sequence data is available as `*.fastq` files. Here is how these files look like:

#### FASTQ

```bash
@NB551191:35:HNWGCBGX3:1:11101:9622:1037 1:N:0:AGGCAGAA+ACTCTAGG
CCTTGNTGCACCTGTGGCATGGAGACCGAATCTTGTGGGGAAACAATCATTTCTTCAGGTCTGAGCTCTCAGATTT
+
AAAAA#EAEEEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEAE/E/AEEEEEEEEEE/EEEEEEEEEEEEEEEEEEE
@NB551191:35:HNWGCBGX3:1:11101:20086:1038 1:N:0:AGGCAGAA+ACTCTAGG
CTGGTNCACCATCCTTGTGTGCTGTTTCATGACAGTAATTACTGAGAGGGTCTGCAATTCAGATCACCTGAAACTC
+
AAAAA#EEEEAEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE
@NB551191:35:HNWGCBGX3:1:11101:4269:1051 1:N:0:AGGCAGAA+ACTGTAGG
TAACAGCACAGAGGATTGAATAAGGTGAGAGCAAAAGTCCTACTACTTATTCAGGCCCCATGTAGCAGTATTCCTC
+
AAAAA6EEEEEEEEEEEEEA/EEEEEEEAEEEEEEE6/EA/EEEEEE/E//EA//EEEEEEEE6A/E6/EE/EEEE
```

The snippet above shows 3 reads. Each read consists of 4 lines:
1. Sequence ID and information
2. The actual read sequence
3. Generally a `+` sign but it could contain additional information
4. The quality scores for each base in line 2

Knowing this, we can calculate the number of reads in a file by counting the number of lines using `wc -l` and dividing by 4:
```bash
zcat -f  ~/Share/physalia_adaptation_course/01_day1/03_raw_reads/BEL-B_12.fq.gz | wc -l 
```
Or using the command below, directly on compressed fastq files (`*.fq.gz`): 
```bash
zcat -f  ~/Share/physalia_adaptation_course/01_day1/03_raw_reads/BEL-B_12.fq.gz | sed -n '2~4p' | wc -l
```

When we have paired-end reads, they are generally stored in two separate files and are paired by position: the first read in `read1.fastq` is paired with the first read in `read2.fastq`, the second read in `read1.fastq` is paired with the second read in `read2.fastq`, and so on. One way to check if your files are intact is to ensure that the two read files have the same amount of lines/reads.

### Conda environment

All the software that you need to run the exercises in our course is installed in a [conda environment](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html). With conda, you can create separate working environments containing software and package versions that you need for a specific analysis. 

To load the environment we use in this course run the following line:
```bash
conda activate adaptg
```

You will see that your prompt now shows the following:
```bash
(adaptg) ubuntu@ip-172-31-16-191:
```

This means the environment is activated and you can run all software just by typing the name in the command line.

>OBS! If you try to run a software and it does not run, remember to check if your conda environment is active.

### Mapping sequence reads to a reference genome
As we are analyzing a large read dataset that could take several hours to align against a reference genome, we did this step for you. You can find a step-by-step breakdown of the script here:

[Script to map reads to a reference genome](script_on_read_mapping.md).

Now, go to your home directory, make a new directory for the alignment files and create a symbolic link to the [BAM](https://samtools.github.io/hts-specs/SAMv1.pdf) files stored in the `~/Share` directory. With a symbolic link, you add the files to a path of your choice, but because they are not physically there you are not using up storage space.
```bash
cd 
mkdir bamfiles
cd bamfiles
ln -s ~/Share/bamfiles/*.bam .

```

#### SAM/BAM
The alignment file contains much more information than the raw data `*.fastq` files. In addition to all the information contained in the `*.fastq` files, we now have information of the quality of the alignment and the position of where those reads mapped on the genome. Although we produce `*.sam` files with the alignment, we quickly convert them to `*.bam`, which is their binary format. These files are not readable directly but you can use these command to visualize them:
```bash
# activate the conda environment (if not done already)
conda activate adaptg

# run samtools to explore the bam file
samtools view -h BEL-B_12.sorted.bam | less 
```
With the pipe sign `|`, we pass the output of the first command into the second command.
The first few lines starting with `@` correspond to the header. If you have a lot of scaffolds in your reference genome, the header could be very long. If you want to see the first few alignments, remove `-h` from the command above.

(exit `less` by pressing `q`).

        
## 1-3. Call variants with Stacks 
[Stacks](http://catchenlab.life.illinois.edu/stacks/) is a bioinformatic pipeline for building loci from short-read data from GBS/[RADseq](https://academic.oup.com/bfg/article/9/5-6/416/182576) libraries and calling variants for population genomics, phylogenomics and linkage maps. 

Stacks includes two main sub-pipelines: 
1. `denovo_map.pl` to build a catalog of loci *de novo* (i.e., it assembles loci from short-read data in absence of a reference genome)

2. `ref_map.pl` to call variants from data mapped to a reference genome. 

Each of the two pipelines includes several steps, which are shown below:
1. `denovo_map.pl` = `ustacks` -> `cstacks` -> `sstacks` -> `tsv2bam` -> `gstacks` -> `populations`

2. `ref_map.pl` = `gstacks` -> `populations`

>Note that all the code here included was tested using Stacks v.2.66.

As you might have guessed already, the first 4 steps in `denovo_map.pl` are for assembling loci from short reads for each individual, and for creating a catalog of all loci across the population when you don't have a reference genome. The next two steps are shared between the two pipelines. `gstacks` builds a catalog of loci and calls SNPs in each sample, and `populations` generates population-level summary statistics and input files for a variety of software for downstream analyses.

As our target species has a reference genome, we will focus on the `ref_map.pl` pipeline. For those interested in the `denovo_map.pl` pipeline, this article is a great resource to select the most appropriate combination of parameters:
>Paris, J. R., Stevens, J. R., & Catchen, J. M. (2017). Lost in parameter space: a road map for stacks. Methods in Ecology and Evolution, 8(10), 1360-1373.

Although the modules `gstacks` and `populations` can be run on one go with the `ref_map.pl` pipeline, we generally call variants in `gstacks` once, and then run `populations` several times to tune-in variant filtering, to print variants for different subsets of samples, and to generate F-statistics for each of these subsets.

Please copy the scripts inside your directory and make executable the one we are interested in:
```bash
# activate the conda environment, if not done already
conda activate adaptg

# come back to your home
cd 

# get inside your scripts
cd scripts 

# now copy files inside your 01_scripts directory (replace the one you may have copied earlier as we edited them)
cp ~/Share/physalia_adaptation_course/01_day1/01_scripts/*.sh 01_scripts/

# make the file executable
# notice that the script name has turned green
chmod +x ~/scripts/01_scripts/stacks_gstacks.sh

# open the file
less ~/scripts/01_scripts/stacks_gstacks.sh

# to exit less, press 'q'
```

Let's have a look at the `stacks_gstacks.sh` script:
```bash
#!/bin/bash
###stacks_gstacks.sh
cd
mkdir -p stacks
cd stacks
mkdir -p gstacks
gstacks -I ~/bamfiles -M ~/scripts/00_documents/popmap_all_day1.txt -O ~/stacks/gstacks -t 3

```
This is simple and self explanatory: we tell the server that we are writing in bash, create a couple of directories (but only if they are not already present with the `-p` flag) and run `gstacks`.

To run this job, we can either use the command `bash` before the script, or make it executable before running it:
```bash
nohup bash ~/scripts/01_scripts/stacks_gstacks.sh >& gstacks.log &
```
Or:
```bash
nohup ~/scripts/01_scripts/stacks_gstacks.sh >& gstacks.log &
```

Note that we use `nohup` to send the job to the background. This allow us to have the Terminal free for us to do other things, and to save the stdout (i.e., the default output stream in a computer program) to a file of our choice.

When the job starts, you can check how it is going from the `gstacks.log` file (using `cat`, `less`, `more`, etc.)

>Note Model: marukilow (var_alpha: 0.01, gt_alpha: 0.05)

For most datasets, the authors recommend the default *marukilow* model, as it takes a Bayesian approach (incorporating information about allele frequencies of the population at each site) and works quite well. 

The *marukihigh* model can call more than two alleles per site, but it does not use a Bayesian approach. The *snp* model is the model by Hohenlohe and collaborators and it was the default model in Stacks v1. Below is Maruki + Lynch's paper to fully understand these models:

>Maruki, T., & Lynch, M. (2017). Genotype Calling from Population-Genomic Sequencing Data. G3: Genes|Genomes|Genetics, 7(5), 1393–1404. 

To expedite things (and while the first gstacks script runs), we can focus on a smaller dataset. This includes only 80 individuals, 40 from two populations in Canada and 40 from 2 populations in Greenland. Run:
```bash
nohup bash ~/scripts/01_scripts/stacks_gstacks_2lin.sh >& gstacks_2lin.log &
```

Once we have our catalog of variants in the `gstacks_2lin` directory, we can run `populations` to apply quality filters and print variant files in different formats. As `populations` is very fast for this small dataset, we can run it from the Terminal. But first, make the directory where you want your results to be saved:
```bash
cd ~/stacks

mkdir populations_2lin_random

populations -P ~/stacks/gstacks_2lin/ -M ~/scripts/popmap_2lin_day1.txt -O ~/stacks/populations_2lin_random -t 4 -p 2 -r 0.8 --fstats --vcf --genepop --structure --write-random-snp

```

Let's break the scripts down to understand what `populations` is doing here:

  `-P` path to the directory containing the Stacks files

  `-M` path to a population map

  `-0` path to a directory where to write the output files

  `-p` minimum number of populations a locus must be present in to process a locus

  `-r` minimum percentage of individuals in a population required to process a locus for that population

  `--fstats --vcf --genepop --structure` for the file formats and stats

  `--write-random-snp` restrict data analysis to one random SNP per locus. This deserves a little more explanation. Most population genetics analyses work under the assumption that your markers are in linkage equilibrium. As you will have multiple SNPs for each locus, and linkage disequilibrium (LD) decays with increasing distance, we select one random SNP for each locus to minimize LD among SNPs. 

It's important that you familiarize with software manuals. Go to <https://catchenlab.life.illinois.edu/stacks/comp/populations.php> and look at the complete list of settings. You can get it also from the Terminal by typing `populations` (or the program command of interest) without additional settings (sometimes `-h` or `--help` are required to visualize the complete list of settings). 

Also note that the files within each directory have very standardized names (check with `ls -lrth populations_2lin_random`). 

>Pro: we don't need to taylor all our scripts to the individual file names, BUT if you don't specify the right directory, you are going to overwrite the existing files causing a lot of confusion on what setting and samples you used for a particular run.

Although we applied stringent filters to minimize the amount of missing data, these are applied at the SNP level, and we could have single individuals that, for a variety of reasons, have a high proportion of missing data (e.g., poor DNA quality, low DNA concentration, pipetting errors). We can check the amount of missing data for each individual with:
```bash
cd populations_2lin_random
# run vcftools to compute missing rates
vcftools --vcf populations.snps.vcf --missing-indv --out missing
# with this you will visualize individuals by % of missing data, from the highest to the lowest
sort -rk5,5 missing.imiss | less 
```
Ideally, % of missing data should be below 5%. 

>Congratulations! You have all the input files ready for downstream analyses (plus a bunch of statistics) for the reduced dataset.

If you do `ls` to list the files in one of the folders storing the output from the `populations` runs, you will notice that Stacks produces several output files. The manual provides a thorough explanation of all the different file formats (<https://catchenlab.life.illinois.edu/stacks/manual/>, Section 6.6).

For each output type, open the corresponding file on the server, and take your time to explore them.

Here you've executed the pipeline on 80 individuals. Before we wrap up today, you need to create the same input files for the full dataset (280 individuals listed in `popmap_all_day1.txt`) and the Canada dataset (240 individuals listed in `popmap_canada_day1.txt`). 

You should have already run the script `~/scripts/gstacks_stacks.sh`. Now, run `populations` on the new catalog twice, once for all individuals, and once for just the Canadian samples. 

```bash
cd ~/stacks
mkdir populations_2lin_random

populations -P ~/stacks/gstacks_2lin/ -M ~/scripts/popmap_2lin_day1.txt -O ~/stacks/populations_2lin_random -t 4 -p 2 -r 0.8 --fstats --vcf --genepop --structure --write-random-snp
```

Make sure you print these outputs in each of these two directories `populations_all_random` and `populations_canada_random`. You might have to run the commands using `nohup`.

If you are struggling with this last step, have a look at the solution [here](populations_script.md).

>**IMPORTANT: Do not use the output files from today during the rest of the course. We will use other inputs.** 

### Variant calling from whole genome re-sequencing data
First, let's set up new folders for these analyses:
```bash
cd
mkdir wgr # for whole genome resequencing data
mkdir wgr/snps_bcftools
```

### SNPs calling
Set symbolic links to the BAM files and their indexes storing aligned reads to reference genome:
```bash
cd wgr
ln -s /home/ubuntu/Share/WGS_bam/*bam* .
# create a list of bam files in a text file
ls *.bam > bams_list.txt 
```

Call variants using [bcftools](https://samtools.github.io/bcftools/):
```bash
bcftools mpileup -Ou -f ~/Share/resources/genome_mallotus_dummy.fasta -b bams_list.txt -r Chr1 -q 5 -I -a AD,DP,SP,ADF,ADR -d 200 | bcftools call - -mv -Ov > snps_bcftools/capelin_wgs_unfiltered.vcf
```
>This step may take a few minutes (~10 min), so it's a good time to take a short break from the computer :)


Variant filtering is done in two steps. First we use `bcftools filter` to filter SNPs based on mapping quality. Then, we apply a series of filters using [VCFtools](https://vcftools.github.io/index.html). Note that we are calling variants for chromosome 1 only to save time:
```bash
cd snps_bcftools

# apply mapping quality filter
bcftools filter -e 'MQ < 30' capelin_wgs_unfiltered.vcf -Ov > capelin_wgs_filtered.tmp.vcf

# count number of unfiltered SNPs surviving filters
grep -v ^\#\# capelin_wgs_filtered.tmp.vcf | wc -l  

# apply other quality filters
vcftools --vcf capelin_wgs_filtered.tmp.vcf \
    --max-alleles 2 \
    --max-missing 0.7 \
    --minDP 3 \
    --maf 0.05 \
    --minQ 30 \
    --minGQ 20 \
    --recode \
    --stdout > capelin_wgs_filtered.vcf 

# count number of SNPs passing filters
grep -v "#" capelin_wgs_filtered.vcf | wc -l

```

Because the variant calling could take some time, you can copy the premade unfiltered clean files to your working directory to explore the different files and play with filtering:
```bash
cp ~/Share/WGS_bam/snps_bcftools/capelin_wgs_*.vcf ~/wgr/snps_bcftools/.

```
>What is the proportion of SNPs surviving filters? How many are left? According to your dataset and needs, you may want to change or tweak these filters.

Finally, prepare the data for downstream analyses by recoding the genotypes in [012 format](https://vcftools.github.io/man_0112a.html):
```bash
vcftools --vcf capelin_wgs_filtered.vcf --012 --out capelin_wgs_filtered

```

**IMPORTANT: As the files used today were generated to be small for quicker analysis, we will use different ones for the rest of the course.** 